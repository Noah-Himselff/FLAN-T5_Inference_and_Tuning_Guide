{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Part 2: Full fine tune of Flan T5 Large"]},{"cell_type":"markdown","metadata":{},"source":["**Table of contents**"]},{"cell_type":"markdown","metadata":{},"source":["- [ 1 - Set up Kernel and Required Dependencies](#1)\n","- [ 2 - Perform Full Fine Tuning](#2)\n","  - [ 2.1 - Tokenizing and Data Process](#2.1)\n","  - [ 2.2 - Model fine tune and training section](#2.2)\n","  - [ 2.3 - Evaluation part 1](#2.3)\n","  - [ 2.4 - Evaluation using rogue metrics](#2.4)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["<a name='1'></a>\n","## 1 - Set up Kernel, Load Required Dependencies, Dataset and LLM"]},{"cell_type":"markdown","metadata":{},"source":["<a name='1.1'></a>\n","### 1.1 - Set up Kernel and Required Dependencies"]},{"cell_type":"markdown","metadata":{"tags":[]},"source":["Now install the required packages for the LLM and datasets.\n","\n","<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1N2M0ZjgiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1N2M0ZjgiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSBhIGZldyBtaW51dGVzIHRvIHJ1bi4gUGxlYXNlIGJlIHBhdGllbnQuPC90ZXh0PgogICAgPHRleHQgeD0iMTAwIiB5PSI1NiIgZm9udC1mYW1pbHk9IkFyaWFsLCBzYW5zLXNlcmlmIiBmb250LXNpemU9IjE0IiBmaWxsPSIjMzMzMzMzIj5JZ25vcmUgdGhlIHdhcm5pbmdzIGFuZCBlcnJvcnMsIGFsb25nIHdpdGggdGhlIG5vdGUgYWJvdXQgcmVzdGFydGluZyB0aGUga2VybmVsIGF0IHRoZSBlbmQuPC90ZXh0Pgo8L3N2Zz4K\" alt=\"Time alert open medium\"/>"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"trusted":true},"outputs":[],"source":["%pip install --upgrade pip\n","%pip install --disable-pip-version-check \\\n","    torch==1.13.1 \\\n","    torchdata==0.5.1 --quiet\n","\n","%pip install \\\n","    transformers==4.27.2 \\\n","    datasets==2.11.0 \\\n","    loralib==0.1.1 \\"]},{"cell_type":"markdown","metadata":{},"source":["As before , i suggest a session restart after the first cell if you are on colab or kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%pip install evaluate==0.4.0 \n","%pip install rouge_score==0.1.2"]},{"cell_type":"markdown","metadata":{"tags":[]},"source":["<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>"]},{"cell_type":"markdown","metadata":{"tags":[]},"source":["Import the necessary components. Some of them are new for this week, they will be discussed later in the notebook. "]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T11:10:29.579565Z","iopub.status.busy":"2024-05-29T11:10:29.578651Z","iopub.status.idle":"2024-05-29T11:10:29.584951Z","shell.execute_reply":"2024-05-29T11:10:29.584023Z","shell.execute_reply.started":"2024-05-29T11:10:29.579523Z"},"tags":[],"trusted":true},"outputs":[],"source":["from datasets import load_dataset\n","from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n","import torch\n","import time\n","import evaluate\n","import pandas as pd\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"tags":[]},"source":["<a name='1.2'></a>\n","### 1.2 - Load Dataset and LLM\n","\n","You are going to continue experimenting with the [DialogSum](https://huggingface.co/datasets/knkarthick/dialogsum) Hugging Face dataset. It contains 10,000+ dialogues with the corresponding manually labeled summaries and topics. "]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T11:10:29.587151Z","iopub.status.busy":"2024-05-29T11:10:29.586439Z","iopub.status.idle":"2024-05-29T11:10:30.672423Z","shell.execute_reply":"2024-05-29T11:10:30.671505Z","shell.execute_reply.started":"2024-05-29T11:10:29.587108Z"},"tags":[],"trusted":true},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'dialogue', 'summary', 'topic'],\n","        num_rows: 12460\n","    })\n","    validation: Dataset({\n","        features: ['id', 'dialogue', 'summary', 'topic'],\n","        num_rows: 500\n","    })\n","    test: Dataset({\n","        features: ['id', 'dialogue', 'summary', 'topic'],\n","        num_rows: 1500\n","    })\n","})"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["huggingface_dataset_name = \"knkarthick/dialogsum\"\n","\n","dataset = load_dataset(huggingface_dataset_name)\n","\n","dataset"]},{"cell_type":"markdown","metadata":{"tags":[]},"source":["Load the pre-trained [FLAN-T5 model](https://huggingface.co/docs/transformers/model_doc/flan-t5) and its tokenizer directly from HuggingFace. Notice that you will be using the [small version](https://huggingface.co/google/flan-t5-base) of FLAN-T5. Setting `torch_dtype=torch.bfloat16` specifies the memory type to be used by this model."]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T11:10:30.674050Z","iopub.status.busy":"2024-05-29T11:10:30.673754Z","iopub.status.idle":"2024-05-29T11:10:34.767477Z","shell.execute_reply":"2024-05-29T11:10:34.766450Z","shell.execute_reply.started":"2024-05-29T11:10:30.674025Z"},"tags":[],"trusted":true},"outputs":[],"source":["model_name='google/flan-t5-large'\n","\n","original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"]},{"cell_type":"markdown","metadata":{"tags":[]},"source":["It is possible to pull out the number of model parameters and find out how many of them are trainable. The following function can be used to do that, at this stage, you do not need to go into details of it. "]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T11:10:34.769209Z","iopub.status.busy":"2024-05-29T11:10:34.768836Z","iopub.status.idle":"2024-05-29T11:10:34.780546Z","shell.execute_reply":"2024-05-29T11:10:34.779531Z","shell.execute_reply.started":"2024-05-29T11:10:34.769175Z"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable model parameters: 783150080\n","all model parameters: 783150080\n","percentage of trainable model parameters: 100.00%\n"]}],"source":["def print_number_of_trainable_model_parameters(model):\n","    trainable_model_params = 0\n","    all_model_params = 0\n","    for _, param in model.named_parameters():\n","        all_model_params += param.numel()\n","        if param.requires_grad:\n","            trainable_model_params += param.numel()\n","    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n","\n","print(print_number_of_trainable_model_parameters(original_model))"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T11:10:34.782139Z","iopub.status.busy":"2024-05-29T11:10:34.781856Z","iopub.status.idle":"2024-05-29T11:10:41.685132Z","shell.execute_reply":"2024-05-29T11:10:41.684009Z","shell.execute_reply.started":"2024-05-29T11:10:34.782108Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["---------------------------------------------------------------------------------------------------\n","INPUT PROMPT:\n","\n","Summarize the following conversation.\n","\n","#Person1#: Have you considered upgrading your system?\n","#Person2#: Yes, but I'm not sure what exactly I would need.\n","#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n","#Person2#: That would be a definite bonus.\n","#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n","#Person2#: How can we do that?\n","#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n","#Person2#: No.\n","#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n","#Person2#: That sounds great. Thanks.\n","\n","Summary:\n","\n","---------------------------------------------------------------------------------------------------\n","BASELINE HUMAN SUMMARY:\n","#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n","\n","---------------------------------------------------------------------------------------------------\n","MODEL GENERATION - ZERO SHOT:\n","#Person2# is considering upgrading her computer. #Person1#: What would you like to do?\n"]}],"source":["index = 200\n","\n","dialogue = dataset['test'][index]['dialogue']\n","summary = dataset['test'][index]['summary']\n","\n","prompt = f\"\"\"\n","Summarize the following conversation.\n","\n","{dialogue}\n","\n","Summary:\n","\"\"\"\n","\n","inputs = tokenizer(prompt, return_tensors='pt')\n","output = tokenizer.decode(\n","    original_model.generate(\n","        inputs[\"input_ids\"], \n","        max_new_tokens=200,\n","    )[0], \n","    skip_special_tokens=True\n",")\n","\n","dash_line = '-'.join('' for x in range(100))\n","print(dash_line)\n","print(f'INPUT PROMPT:\\n{prompt}')\n","print(dash_line)\n","print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n","print(dash_line)\n","print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"]},{"cell_type":"markdown","metadata":{"tags":[]},"source":["<a name='2'></a>\n","## 2 - Perform Full Fine-Tuning"]},{"cell_type":"markdown","metadata":{},"source":["<a name='2.1'></a>\n","## 2.1 - Tokenizing Data"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T11:10:41.687472Z","iopub.status.busy":"2024-05-29T11:10:41.686786Z","iopub.status.idle":"2024-05-29T11:10:57.418219Z","shell.execute_reply":"2024-05-29T11:10:57.417496Z","shell.execute_reply.started":"2024-05-29T11:10:41.687430Z"},"tags":[],"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2aabb6f42879422ba3eb22f5698fbbea","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/12460 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2641ba1a656340e890cee2b49111da91","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/500 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"36c02d9aa5ce4f219ed11f18a5605649","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["def tokenize_function(example):\n","    start_prompt = 'Summarize the following conversation.\\n\\n'\n","    end_prompt = '\\n\\nSummary: '\n","    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n","    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n","    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n","    \n","    return example\n","\n","# The dataset actually contains 3 diff splits: train, validation, test.\n","# The tokenize_function code is handling all data across all splits in batches.\n","tokenized_datasets = dataset.map(tokenize_function, batched=True)\n","tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])"]},{"cell_type":"markdown","metadata":{"tags":[]},"source":["To save some time in the lab, you will subsample the dataset:"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T11:10:57.421636Z","iopub.status.busy":"2024-05-29T11:10:57.421343Z","iopub.status.idle":"2024-05-29T11:11:07.183434Z","shell.execute_reply":"2024-05-29T11:11:07.182539Z","shell.execute_reply.started":"2024-05-29T11:10:57.421612Z"},"tags":[],"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"04c5f6236c9640b486608cc574514c6c","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/12460 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ff1b2350d3fa476bb1fe9a56d283607c","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/500 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"052326e1f1004e23819367bdd0567d69","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/1500 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)"]},{"cell_type":"markdown","metadata":{"tags":[]},"source":["Check the shapes of all three parts of the dataset:"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T11:11:07.184686Z","iopub.status.busy":"2024-05-29T11:11:07.184436Z","iopub.status.idle":"2024-05-29T11:11:07.190446Z","shell.execute_reply":"2024-05-29T11:11:07.189510Z","shell.execute_reply.started":"2024-05-29T11:11:07.184664Z"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Shapes of the datasets:\n","Training: (125, 2)\n","Validation: (5, 2)\n","Test: (15, 2)\n","DatasetDict({\n","    train: Dataset({\n","        features: ['input_ids', 'labels'],\n","        num_rows: 125\n","    })\n","    validation: Dataset({\n","        features: ['input_ids', 'labels'],\n","        num_rows: 5\n","    })\n","    test: Dataset({\n","        features: ['input_ids', 'labels'],\n","        num_rows: 15\n","    })\n","})\n"]}],"source":["print(f\"Shapes of the datasets:\")\n","print(f\"Training: {tokenized_datasets['train'].shape}\")\n","print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n","print(f\"Test: {tokenized_datasets['test'].shape}\")\n","\n","print(tokenized_datasets)"]},{"cell_type":"markdown","metadata":{},"source":["The output dataset is ready for fine-tuning."]},{"cell_type":"markdown","metadata":{},"source":["<a name='2.2'></a>\n","### 2.2 - Fine-Tune the Model with the Preprocessed Dataset\n","\n","Now utilize the built-in Hugging Face `Trainer` class (see the documentation [here](https://huggingface.co/docs/transformers/main_classes/trainer)). Pass the preprocessed dataset with reference to the original model. Other training parameters are found experimentally and there is no need to go into details about those at the moment.\n","   For easier access and hyperparameter tuning i put all the 2.2 part into a single cell"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from transformers import Trainer, TrainingArguments, T5ForConditionalGeneration, DataCollatorForSeq2Seq\n","import time\n","import torch\n","\n","# Ensure that PyTorch recognizes multiple GPUs\n","print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n","\n","# Define output directory with a timestamp\n","output_dir = f'fully fine tuned test 50e 6e-5'\n","\n","# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir=output_dir,\n","    learning_rate=8e-5,\n","    per_device_train_batch_size=2,  # Reduce batch size if still encountering OOM issues\n","    per_device_eval_batch_size=2,\n","    num_train_epochs=50,\n","    weight_decay=0.01,\n","    logging_steps=1,\n","    #fp16=True,  # Enable mixed precision\n","    #dataloader_drop_last=True,  # Drop the last incomplete batch\n","    #ddp_find_unused_parameters=False,  # Avoid unnecessary parameter reduction in DDP\n",")\n","\n","# Data collator for dynamic padding\n","data_collator = DataCollatorForSeq2Seq(tokenizer, model=original_model, padding=True)\n","\n","# Initialize Trainer\n","trainer = Trainer(\n","    model=original_model,\n","    args=training_args,\n","    train_dataset=tokenized_datasets['train'],\n","    eval_dataset=tokenized_datasets['validation'],\n","    data_collator=data_collator\n",")\n","\n","# Clear CUDA cache\n","torch.cuda.empty_cache()\n","\n","# Train the model\n","trainer.train()\n","trainer.save_model(output_dir)\n","\n","instruct_model = AutoModelForSeq2SeqLM.from_pretrained(output_dir, torch_dtype=torch.bfloat16)\n","\n","device = torch.device(\"cpu\")\n","original_model.to(device)\n","instruct_model.to(device)\n","index = 200\n","dialogue = dataset['test'][index]['dialogue']\n","human_baseline_summary = dataset['test'][index]['summary']\n","\n","prompt = f\"\"\"\n","Summarize the following conversation.\n","\n","{dialogue}\n","\n","Summary:\n","\"\"\"\n","\n","input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n","\n","original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n","original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n","\n","instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n","instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n","\n","print(dash_line)\n","print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n","print(dash_line)\n","print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n","print(dash_line)\n","print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')\n","\n","rouge = evaluate.load('rouge')\n","dialogues = dataset['test'][0:10]['dialogue']\n","human_baseline_summaries = dataset['test'][0:10]['summary']\n","\n","original_model_summaries = []\n","instruct_model_summaries = []\n","\n","for _, dialogue in enumerate(dialogues):\n","    prompt = f\"\"\"\n","Summarize the following conversation.\n","\n","{dialogue}\n","\n","Summary: \"\"\"\n","    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n","\n","    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n","    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n","    original_model_summaries.append(original_model_text_output)\n","\n","    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n","    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n","    instruct_model_summaries.append(instruct_model_text_output)\n","    \n","zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries))\n"," \n","df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries'])\n","df\n","\n","original_model_results = rouge.compute(\n","    predictions=original_model_summaries,\n","    references=human_baseline_summaries[0:len(original_model_summaries)],\n","    use_aggregator=True,\n","    use_stemmer=True,\n",")\n","\n","instruct_model_results = rouge.compute(\n","    predictions=instruct_model_summaries,\n","    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n","    use_aggregator=True,\n","    use_stemmer=True,\n",")\n","\n","print('ORIGINAL MODEL:')\n","print(original_model_results)\n","print('INSTRUCT MODEL:')\n","print(instruct_model_results)"]},{"cell_type":"markdown","metadata":{"tags":[]},"source":["Create an instance of the `AutoModelForSeq2SeqLM` class for the instruct model:"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T17:06:48.757759Z","iopub.status.busy":"2024-05-27T17:06:48.757419Z","iopub.status.idle":"2024-05-27T17:06:52.017058Z","shell.execute_reply":"2024-05-27T17:06:52.016019Z","shell.execute_reply.started":"2024-05-27T17:06:48.757735Z"},"tags":[],"trusted":true},"outputs":[],"source":["instruct_model = AutoModelForSeq2SeqLM.from_pretrained('/kaggle/working/fully fine tuned test 50e 6e-5', torch_dtype=torch.bfloat16)"]},{"cell_type":"markdown","metadata":{},"source":["<a name='2.3'></a>\n","### 2.3 - Evaluate the Model Qualitatively (Human Evaluation)\n","\n","As with many GenAI applications, a qualitative approach where you ask yourself the question \"Is my model behaving the way it is supposed to?\" is usually a good starting point. In the example below (the same one we started this notebook with), you can see how the fine-tuned model is able to create a reasonable summary of the dialogue compared to the original inability to understand what is being asked of the model."]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T17:07:00.137553Z","iopub.status.busy":"2024-05-27T17:07:00.136704Z","iopub.status.idle":"2024-05-27T17:07:11.089601Z","shell.execute_reply":"2024-05-27T17:07:11.088625Z","shell.execute_reply.started":"2024-05-27T17:07:00.137520Z"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["---------------------------------------------------------------------------------------------------\n","BASELINE HUMAN SUMMARY:\n","#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n","---------------------------------------------------------------------------------------------------\n","ORIGINAL MODEL:\n","A computer user wants to upgrade his computer.\n","---------------------------------------------------------------------------------------------------\n","INSTRUCT MODEL:\n","The computer is outdated. The computer needs a painting program. The computer needs a CD-ROM drive.\n"]}],"source":["device = torch.device(\"cpu\")\n","original_model.to(device)\n","instruct_model.to(device)\n","index = 200\n","dialogue = dataset['test'][index]['dialogue']\n","human_baseline_summary = dataset['test'][index]['summary']\n","\n","prompt = f\"\"\"\n","Summarize the following conversation.\n","\n","{dialogue}\n","\n","Summary:\n","\"\"\"\n","\n","input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n","\n","original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n","original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n","\n","instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n","instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n","\n","print(dash_line)\n","print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n","print(dash_line)\n","print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n","print(dash_line)\n","print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')"]},{"cell_type":"markdown","metadata":{},"source":["<a name='2.4'></a>\n","### 2.4 - Evaluate the Model Quantitatively (with ROUGE Metric)\n","\n","The [ROUGE metric](https://en.wikipedia.org/wiki/ROUGE_(metric)) helps quantify the validity of summarizations produced by models. It compares summarizations to a \"baseline\" summary which is usually created by a human. While not perfect, it does indicate the overall increase in summarization effectiveness that we have accomplished by fine-tuning."]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T22:15:44.118354Z","iopub.status.busy":"2024-05-25T22:15:44.117921Z","iopub.status.idle":"2024-05-25T22:15:44.358720Z","shell.execute_reply":"2024-05-25T22:15:44.357610Z","shell.execute_reply.started":"2024-05-25T22:15:44.118320Z"},"tags":[],"trusted":true},"outputs":[],"source":["rouge = evaluate.load('rouge')"]},{"cell_type":"markdown","metadata":{},"source":["Generate the outputs for the sample of the test dataset (only 10 dialogues and summaries to save time), and save the results."]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T22:15:45.300914Z","iopub.status.busy":"2024-05-25T22:15:45.300403Z","iopub.status.idle":"2024-05-25T22:16:28.807943Z","shell.execute_reply":"2024-05-25T22:16:28.806270Z","shell.execute_reply.started":"2024-05-25T22:15:45.300869Z"},"tags":[],"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>human_baseline_summaries</th>\n","      <th>original_model_summaries</th>\n","      <th>instruct_model_summaries</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n","      <td>The memo will be distributed to all employees ...</td>\n","      <td>This memo is to be distributed to all employee...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>In order to prevent employees from wasting tim...</td>\n","      <td>The Office of the Secretary-Treasury is underg...</td>\n","      <td>This memo is to be distributed to all employee...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n","      <td>#Person1#: This memo is for the use of intra-o...</td>\n","      <td>This memo is to be distributed to all employee...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>#Person2# arrives late because of traffic jam....</td>\n","      <td>The driver is a little nervous about the traff...</td>\n","      <td>Taking public transport to work is a good idea.</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n","      <td>The following people are discussing the follow...</td>\n","      <td>Taking public transport to work is a good idea.</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>#Person2# complains to #Person1# about the tra...</td>\n","      <td>The traffic is congested in the city.</td>\n","      <td>Taking public transport to work is a good idea.</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n","      <td>Masha and Hero are getting divorced.</td>\n","      <td>Masha and Hero are getting divorced.</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n","      <td>#Person1: Masha and Hero are getting divorced....</td>\n","      <td>Masha and Hero are getting divorced.</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>#Person1# and Kate talk about the divorce betw...</td>\n","      <td>Masha and Hero are getting divorced.</td>\n","      <td>Masha and Hero are getting divorced.</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>#Person1# and Brian are at the birthday party ...</td>\n","      <td>People at a party in London have a party.</td>\n","      <td>Brian's birthday is coming up.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                            human_baseline_summaries  \\\n","0  Ms. Dawson helps #Person1# to write a memo to ...   \n","1  In order to prevent employees from wasting tim...   \n","2  Ms. Dawson takes a dictation for #Person1# abo...   \n","3  #Person2# arrives late because of traffic jam....   \n","4  #Person2# decides to follow #Person1#'s sugges...   \n","5  #Person2# complains to #Person1# about the tra...   \n","6  #Person1# tells Kate that Masha and Hero get d...   \n","7  #Person1# tells Kate that Masha and Hero are g...   \n","8  #Person1# and Kate talk about the divorce betw...   \n","9  #Person1# and Brian are at the birthday party ...   \n","\n","                            original_model_summaries  \\\n","0  The memo will be distributed to all employees ...   \n","1  The Office of the Secretary-Treasury is underg...   \n","2  #Person1#: This memo is for the use of intra-o...   \n","3  The driver is a little nervous about the traff...   \n","4  The following people are discussing the follow...   \n","5              The traffic is congested in the city.   \n","6               Masha and Hero are getting divorced.   \n","7  #Person1: Masha and Hero are getting divorced....   \n","8               Masha and Hero are getting divorced.   \n","9          People at a party in London have a party.   \n","\n","                            instruct_model_summaries  \n","0  This memo is to be distributed to all employee...  \n","1  This memo is to be distributed to all employee...  \n","2  This memo is to be distributed to all employee...  \n","3    Taking public transport to work is a good idea.  \n","4    Taking public transport to work is a good idea.  \n","5    Taking public transport to work is a good idea.  \n","6               Masha and Hero are getting divorced.  \n","7               Masha and Hero are getting divorced.  \n","8               Masha and Hero are getting divorced.  \n","9                     Brian's birthday is coming up.  "]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["dialogues = dataset['test'][0:10]['dialogue']\n","human_baseline_summaries = dataset['test'][0:10]['summary']\n","\n","original_model_summaries = []\n","instruct_model_summaries = []\n","\n","for _, dialogue in enumerate(dialogues):\n","    prompt = f\"\"\"\n","Summarize the following conversation.\n","\n","{dialogue}\n","\n","Summary: \"\"\"\n","    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n","\n","    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n","    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n","    original_model_summaries.append(original_model_text_output)\n","\n","    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n","    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n","    instruct_model_summaries.append(instruct_model_text_output)\n","    \n","zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries))\n"," \n","df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries'])\n","df"]},{"cell_type":"markdown","metadata":{"tags":[]},"source":["Evaluate the models computing ROUGE metrics. Notice the improvement in the results!"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T22:16:28.811341Z","iopub.status.busy":"2024-05-25T22:16:28.810186Z","iopub.status.idle":"2024-05-25T22:16:29.283514Z","shell.execute_reply":"2024-05-25T22:16:29.282679Z","shell.execute_reply.started":"2024-05-25T22:16:28.811301Z"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["ORIGINAL MODEL:\n","{'rouge1': 0.25141479159303737, 'rouge2': 0.08737967999951035, 'rougeL': 0.20580064835952133, 'rougeLsum': 0.20794850614762303}\n","INSTRUCT MODEL:\n","{'rouge1': 0.28647748040489973, 'rouge2': 0.13497482028216662, 'rougeL': 0.23619027925479535, 'rougeLsum': 0.23930701616185485}\n"]}],"source":["original_model_results = rouge.compute(\n","    predictions=original_model_summaries,\n","    references=human_baseline_summaries[0:len(original_model_summaries)],\n","    use_aggregator=True,\n","    use_stemmer=True,\n",")\n","\n","instruct_model_results = rouge.compute(\n","    predictions=instruct_model_summaries,\n","    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n","    use_aggregator=True,\n","    use_stemmer=True,\n",")\n","\n","print('ORIGINAL MODEL:')\n","print(original_model_results)\n","print('INSTRUCT MODEL:')\n","print(instruct_model_results)"]},{"cell_type":"markdown","metadata":{},"source":["The file `data/dialogue-summary-training-results.csv` contains a pre-populated list of all model results which you can use to evaluate on a larger section of data. Let's do that for each of the models:"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[]},"outputs":[],"source":["results = pd.read_csv(\"data/dialogue-summary-training-results.csv\")\n","\n","human_baseline_summaries = results['human_baseline_summaries'].values\n","original_model_summaries = results['original_model_summaries'].values\n","instruct_model_summaries = results['instruct_model_summaries'].values\n","\n","original_model_results = rouge.compute(\n","    predictions=original_model_summaries,\n","    references=human_baseline_summaries[0:len(original_model_summaries)],\n","    use_aggregator=True,\n","    use_stemmer=True,\n",")\n","\n","instruct_model_results = rouge.compute(\n","    predictions=instruct_model_summaries,\n","    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n","    use_aggregator=True,\n","    use_stemmer=True,\n",")\n","\n","print('ORIGINAL MODEL:')\n","print(original_model_results)\n","print('INSTRUCT MODEL:')\n","print(instruct_model_results)"]},{"cell_type":"markdown","metadata":{"tags":[]},"source":["The results show substantial improvement in all ROUGE metrics:"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-27T17:09:05.173886Z","iopub.status.busy":"2024-05-27T17:09:05.173526Z","iopub.status.idle":"2024-05-27T17:09:05.183930Z","shell.execute_reply":"2024-05-27T17:09:05.182990Z","shell.execute_reply.started":"2024-05-27T17:09:05.173858Z"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Absolute percentage improvement of INSTRUCT MODEL over HUMAN BASELINE\n","rouge1: 13.33%\n","rouge2: 7.33%\n","rougeL: 8.67%\n","rougeLsum: 8.38%\n"]}],"source":["print(\"Absolute percentage improvement of INSTRUCT MODEL over HUMAN BASELINE\")\n","\n","improvement = (np.array(list(instruct_model_results.values())) - np.array(list(original_model_results.values())))\n","for key, value in zip(instruct_model_results.keys(), improvement):\n","    print(f'{key}: {value*100:.2f}%')"]}],"metadata":{"availableInstances":[{"_defaultOrder":0,"_isFastLaunch":true,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":4,"name":"ml.t3.medium","vcpuNum":2},{"_defaultOrder":1,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.t3.large","vcpuNum":2},{"_defaultOrder":2,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.t3.xlarge","vcpuNum":4},{"_defaultOrder":3,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.t3.2xlarge","vcpuNum":8},{"_defaultOrder":4,"_isFastLaunch":true,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.m5.large","vcpuNum":2},{"_defaultOrder":5,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.m5.xlarge","vcpuNum":4},{"_defaultOrder":6,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.m5.2xlarge","vcpuNum":8},{"_defaultOrder":7,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.m5.4xlarge","vcpuNum":16},{"_defaultOrder":8,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.m5.8xlarge","vcpuNum":32},{"_defaultOrder":9,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.m5.12xlarge","vcpuNum":48},{"_defaultOrder":10,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.m5.16xlarge","vcpuNum":64},{"_defaultOrder":11,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.m5.24xlarge","vcpuNum":96},{"_defaultOrder":12,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.m5d.large","vcpuNum":2},{"_defaultOrder":13,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.m5d.xlarge","vcpuNum":4},{"_defaultOrder":14,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.m5d.2xlarge","vcpuNum":8},{"_defaultOrder":15,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.m5d.4xlarge","vcpuNum":16},{"_defaultOrder":16,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.m5d.8xlarge","vcpuNum":32},{"_defaultOrder":17,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.m5d.12xlarge","vcpuNum":48},{"_defaultOrder":18,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.m5d.16xlarge","vcpuNum":64},{"_defaultOrder":19,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.m5d.24xlarge","vcpuNum":96},{"_defaultOrder":20,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":true,"memoryGiB":0,"name":"ml.geospatial.interactive","supportedImageNames":["sagemaker-geospatial-v1-0"],"vcpuNum":0},{"_defaultOrder":21,"_isFastLaunch":true,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":4,"name":"ml.c5.large","vcpuNum":2},{"_defaultOrder":22,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.c5.xlarge","vcpuNum":4},{"_defaultOrder":23,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.c5.2xlarge","vcpuNum":8},{"_defaultOrder":24,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.c5.4xlarge","vcpuNum":16},{"_defaultOrder":25,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":72,"name":"ml.c5.9xlarge","vcpuNum":36},{"_defaultOrder":26,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":96,"name":"ml.c5.12xlarge","vcpuNum":48},{"_defaultOrder":27,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":144,"name":"ml.c5.18xlarge","vcpuNum":72},{"_defaultOrder":28,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.c5.24xlarge","vcpuNum":96},{"_defaultOrder":29,"_isFastLaunch":true,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.g4dn.xlarge","vcpuNum":4},{"_defaultOrder":30,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.g4dn.2xlarge","vcpuNum":8},{"_defaultOrder":31,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.g4dn.4xlarge","vcpuNum":16},{"_defaultOrder":32,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.g4dn.8xlarge","vcpuNum":32},{"_defaultOrder":33,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.g4dn.12xlarge","vcpuNum":48},{"_defaultOrder":34,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.g4dn.16xlarge","vcpuNum":64},{"_defaultOrder":35,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":61,"name":"ml.p3.2xlarge","vcpuNum":8},{"_defaultOrder":36,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":244,"name":"ml.p3.8xlarge","vcpuNum":32},{"_defaultOrder":37,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":488,"name":"ml.p3.16xlarge","vcpuNum":64},{"_defaultOrder":38,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":768,"name":"ml.p3dn.24xlarge","vcpuNum":96},{"_defaultOrder":39,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.r5.large","vcpuNum":2},{"_defaultOrder":40,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.r5.xlarge","vcpuNum":4},{"_defaultOrder":41,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.r5.2xlarge","vcpuNum":8},{"_defaultOrder":42,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.r5.4xlarge","vcpuNum":16},{"_defaultOrder":43,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.r5.8xlarge","vcpuNum":32},{"_defaultOrder":44,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.r5.12xlarge","vcpuNum":48},{"_defaultOrder":45,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":512,"name":"ml.r5.16xlarge","vcpuNum":64},{"_defaultOrder":46,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":768,"name":"ml.r5.24xlarge","vcpuNum":96},{"_defaultOrder":47,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.g5.xlarge","vcpuNum":4},{"_defaultOrder":48,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.g5.2xlarge","vcpuNum":8},{"_defaultOrder":49,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.g5.4xlarge","vcpuNum":16},{"_defaultOrder":50,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.g5.8xlarge","vcpuNum":32},{"_defaultOrder":51,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.g5.16xlarge","vcpuNum":64},{"_defaultOrder":52,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.g5.12xlarge","vcpuNum":48},{"_defaultOrder":53,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.g5.24xlarge","vcpuNum":96},{"_defaultOrder":54,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":768,"name":"ml.g5.48xlarge","vcpuNum":192},{"_defaultOrder":55,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":1152,"name":"ml.p4d.24xlarge","vcpuNum":96},{"_defaultOrder":56,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":1152,"name":"ml.p4de.24xlarge","vcpuNum":96}],"colab":{"name":"Fine-tune a language model","provenance":[]},"instance_type":"ml.m5.2xlarge","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":4}
